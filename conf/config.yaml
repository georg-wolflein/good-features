defaults:
  - _self_
  - dataset@dataset: tcga_brca_subtypes
  - augmentations@dataset.augmentations: none

settings:
  feature_dim: 768
  feature_extractor: ctranspath

name: train-${dataset.name}-${model_name:${model._target_}}-${settings.feature_extractor}
project: histaug
output_dir: /data/histaug/results/train/${dataset.name}
seed: 0 # leave empty to use random seed
dataset: # some more options will be added by the dataset config in conf/dataset
  augmentations: # will be overwritten by augmentations from conf/augmentations
  batch_size: 32
  instances_per_bag: 4096
  num_workers: 32
accumulate_grad_samples: #8
model:
  # _target_: histaug.train.models.MeanAveragePooling
  _target_: histaug.train.models.AttentionMIL
  d_features: 768
  hidden_dim: 512
  targets: ${dataset.targets}
  batchnorm: true
deploy:
  checkpoint: #/mnt/bulk/gwoelflein/georg-transformers/output/mq1jcaq8/lightning_logs/version_0/checkpoints/last.ckpt
tune_lr: false
optimizer:
  _target_: torch.optim.AdamW
  lr: 3e-4
scheduler:
  # _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  # T_max: ${max_epochs}
early_stopping:
  metric: val/${dataset.targets[0].column}/auroc
  goal: max
  patience: 10
  enabled: false
restore_best_checkpoint: true
max_epochs: 50
grad_clip: .5
device: # leave empty to use 1 GPU