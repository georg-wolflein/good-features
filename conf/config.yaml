defaults:
  - _self_
  - dataset@dataset: camelyon16_train
  - augmentations@dataset.augmentations: none

settings:
  feature_dim: 768
  feature_extractor: ctranspath

name: ${dataset.name}-${model_name:${model._target_}}-${settings.feature_extractor}
project: histaug
output_dir: /data/histaug/results/train/${dataset.name}
seed: 0 # leave empty to use random seed
dataset: # some more options will be added by the dataset config in conf/dataset
  augmentations: # will be overwritten by augmentations from conf/augmentations
  batch_size: 1
  instances_per_bag: 8192
  num_workers: 16
accumulate_grad_samples: 4
model:
  # _target_: histaug.train.models.MeanAveragePooling
  _target_: histaug.train.models.AttentionMIL
  d_features: ${settings.feature_dim}
  hidden_dim: 256
  targets: ${dataset.targets}
  batchnorm: false
deploy:
  checkpoint: #/mnt/bulk/gwoelflein/georg-transformers/output/mq1jcaq8/lightning_logs/version_0/checkpoints/last.ckpt
tune_lr: false
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 1e-2
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${max_epochs}
early_stopping:
  metric: val/${dataset.targets[0].column}/auroc
  goal: max
  patience: 10
  enabled: false
restore_best_checkpoint: true
max_epochs: 30
grad_clip: #.5
device: # leave empty to use 1 GPU