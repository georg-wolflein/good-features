defaults:
  - _self_
  - dataset@dataset: tcga_brca_subtype
  - test.dataset@dataset: cptac_brca_subtype
  - augmentations@dataset.augmentations: none

settings:
  feature_dim: 768
  feature_extractor: ctranspath

name: ${dataset.name}-${model_name:${model._target_}}-${settings.feature_extractor}
project: histaug
output_dir: /data/histaug/train/${dataset.name}
seed: 0 # leave empty to use random seed
dataset: # some more options will be added by the dataset config in conf/dataset
  augmentations: # will be overwritten by augmentations from conf/augmentations
  batch_size: 1
  instances_per_bag: 8192
  num_workers: 16
accumulate_grad_samples: 4
model:
  # _target_: histaug.train.models.MeanAveragePooling
  _target_: histaug.train.models.AttentionMIL
  d_features: ${settings.feature_dim}
  hidden_dim: 256
  targets: ${dataset.targets}
  batchnorm: false
test:
  enabled: true
  dataset: # some more options will be added by the dataset config in conf/dataset
    augmentations: ${dataset.augmentations.val}
    batch_size: ${dataset.batch_size}
    instances_per_bag: ${dataset.instances_per_bag}
    num_workers: ${dataset.num_workers}
tune_lr: false
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 1e-2
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${max_epochs}
early_stopping:
  # metric: val/${dataset.targets[0].column}/auroc
  # goal: max
  metric: val/loss
  goal: min
  patience: 10
  enabled: true
restore_best_checkpoint: true
max_epochs: 30
grad_clip: #.5
device: # leave empty to use 1 GPU